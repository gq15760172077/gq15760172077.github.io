(window.webpackJsonp=window.webpackJsonp||[]).push([[34],{514:function(t,e,n){"use strict";n.r(e);var r=n(4),i=Object(r.a)({},(function(){var t=this,e=t.$createElement,n=t._self._c||e;return n("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[n("h2",{attrs:{id:"深度迁移学习"}},[n("a",{staticClass:"header-anchor",attrs:{href:"#深度迁移学习"}},[t._v("#")]),t._v(" 深度迁移学习")]),t._v(" "),n("ul",[n("li",[t._v("目的\n"),n("ul",[n("li",[t._v("解决word Embedding的不足， word Embedding最大的不足是无法解决不同词在不同的语境中编码一致的问题")]),t._v(" "),n("li",[t._v("充分利用无标注数据 （希望不需要人去标注）")]),t._v(" "),n("li",[t._v("使用较深的模型 （之前NLP的网络不会太深）")]),t._v(" "),n("li",[t._v("训练的时候用更少的数据（从其它任务迁移出一部分特征作为预训练）")])])]),t._v(" "),n("li",[t._v("ELMo"),n("br"),t._v("\n大致是先经过character embedding，再经过convolution，最终输入到一个双向LSTM中，其用法主要是拿出LSTM中的隐藏层，然后与word embedding去作拼接或求和等再输入到原本需要用到word embedding的位置"),n("br"),t._v("\n解决了前面三个目的（除了不能使用较深的模型）。")]),t._v(" "),n("li",[t._v("BERT")])])])}),[],!1,null,null,null);e.default=i.exports}}]);